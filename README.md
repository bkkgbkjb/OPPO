# Beyond Reward: Offline Preference-guided Policy Optimization (OPPO)

This is source code for the paper, ["Beyond Reward: Offline Preference-guided Policy Optimization"](https://openreview.net/forum?id=0BgDXE6vJJ&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICML.cc%2F2023%2FConference%2FAuthors%23your-submissions)) (`OPPO`)

Main codes are in `oppo` folder
It contains 2 parts: 

`scripted` contains code to reproduce results using preferences generated by a "scripted teacher".

`human` contains code to train/eval `OPPO` using human-labeled perference, which is from [`Preference Transformer`](https://openreview.net/forum?id=Peot1SFDX0), please refer to their [codebase](https://www.google.com/search?q=preference+trasnformer+github&oq=preference+trasnformer+github&aqs=chrome..69i57j0i13i512j69i64j0i390i650l3.1975j1j7&sourceid=chrome&ie=UTF-8) for further details and consider cite their [paper](https://openreview.net/forum?id=Peot1SFDX0) if needed


# Citation
```

```

# Acknowledgements

Our code is largely based on [Decision Transformer](https://github.com/kzl/decision-transformer)

Human labels are obtained thanks to [Preference Transformer](https://openreview.net/forum?id=Peot1SFDX0)

Our experiments, largely used [D4RL](https://github.com/Farama-Foundation/D4RL) dataset

`Lift` and `Can` environments are owing to [Robomimic](https://robomimic.github.io/) and [Robosuite](https://robosuite.ai/) project